# Ability to override ceph.conf
#configOverride: |
#  [osd.3]
#  osd_memory_target_cgroup_limit_ratio = 1.0


toolbox:
  enabled: true


monitoring:
  # requires Prometheus to be pre-installed
  # enabling will also create RBAC rules to allow Operator to create ServiceMonitors
  enabled: false
  rulesNamespaceOverride:

# All values below are taken from the CephCluster CRD
# More information can be found at [Ceph Cluster CRD](/Documentation/ceph-cluster-crd.md)
cephClusterSpec:
  mon:
    # Set the number of mons to be started. Must be an odd number, and is generally recommended to be 3.
    count: 3

  mgr:
    # When higher availability of the mgr is needed, increase the count to 2.
    # In that case, one mgr will be active and one in standby. When Ceph updates which
    # mgr is active, Rook will update the mgr services to match the active mgr.
    count: 2
    modules:
      # Several modules should not need to be included in this list. The "dashboard" and "monitoring" modules
      # are already enabled by other settings in the cluster CR.
      - name: pg_autoscaler
        enabled: true

  # automate [data cleanup process](https://github.com/rook/rook/blob/master/Documentation/ceph-teardown.md#delete-the-data-on-hosts) in cluster destruction.
  cleanupPolicy:
    # Since cluster cleanup is destructive to data, confirmation is required.
    # To destroy all Rook data on hosts during uninstall, confirmation must be set to "yes-really-destroy-data".
    # This value should only be set when the cluster is about to be deleted. After the confirmation is set,
    # Rook will immediately stop configuring the cluster and only wait for the delete command.
    # If the empty string is set, Rook will not destroy any data on hosts during uninstall.
    confirmation: ""
    # sanitizeDisks represents settings for sanitizing OSD disks on cluster deletion
    sanitizeDisks:
      # method indicates if the entire disk should be sanitized or simply ceph's metadata
      # in both case, re-install is possible
      # possible choices are 'complete' or 'quick' (default)
      method: quick
      # dataSource indicate where to get random bytes from to write on the disk
      # possible choices are 'zero' (default) or 'random'
      # using random sources will consume entropy from the system and will take much more time then the zero source
      dataSource: zero
      # iteration overwrite N times instead of the default (1)
      # takes an integer value
      iteration: 1
    # allowUninstallWithVolumes defines how the uninstall should be performed
    # If set to true, cephCluster deletion does not wait for the PVs to be deleted.
    allowUninstallWithVolumes: false

  # The option to automatically remove OSDs that are out and are safe to destroy.
  removeOSDsIfOutAndSafeToRemove: true

dashboard:
  enabled: true
  # serve the dashboard under a subpath (useful when you are accessing the dashboard via a reverse proxy)
  urlPrefix: ""
  # serve the dashboard at the given port.
  port: 8443
  ssl: false

ingress:
  dashboard:
    annotations:
      kubernetes.io/ingress.class: traefik
      traefik.ingress.kubernetes.io/router.entrypoints: websecure
      cert-manager.io/issuer: cloudflare-origin-issuer
      cert-manager.io/issuer-kind: OriginIssuer
      cert-manager.io/issuer-group: cert-manager.k8s.cloudflare.com
      cert-manager.io/duration: 168h
      cert-manager.io/renew-before: 24h
    host:
      name: ceph.bhyoo.com
      path: /
      pathType: Prefix
    tls:
      - hosts:
          - ceph.bhyoo.com
        secretName: ceph-com

cephBlockPools:
#  - name: ceph-blockpool
#    # see https://github.com/rook/rook/blob/master/Documentation/ceph-pool-crd.md#spec for available configuration
#    spec:
#      failureDomain: host
#      replicated:
#        size: 3
#    storageClass:
#      enabled: true
#      name: ceph-block
#      isDefault: true
#      reclaimPolicy: Delete
#      allowVolumeExpansion: true
#      # see https://github.com/rook/rook/blob/master/Documentation/ceph-block.md#provision-storage for available configuration
#      parameters:
#        # (optional) mapOptions is a comma-separated list of map options.
#        # For krbd options refer
#        # https://docs.ceph.com/docs/master/man/8/rbd/#kernel-rbd-krbd-options
#        # For nbd options refer
#        # https://docs.ceph.com/docs/master/man/8/rbd-nbd/#options
#        # mapOptions: lock_on_read,queue_depth=1024
#
#        # (optional) unmapOptions is a comma-separated list of unmap options.
#        # For krbd options refer
#        # https://docs.ceph.com/docs/master/man/8/rbd/#kernel-rbd-krbd-options
#        # For nbd options refer
#        # https://docs.ceph.com/docs/master/man/8/rbd-nbd/#options
#        # unmapOptions: force
#
#        # RBD image format. Defaults to "2".
#        imageFormat: "2"
#        # RBD image features. Available for imageFormat: "2". CSI RBD currently supports only `layering` feature.
#        imageFeatures: layering
#        # The secrets contain Ceph admin credentials.
#        csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
#        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
#        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
#        csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
#        csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
#        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
#        # Specify the filesystem type of the volume. If not specified, csi-provisioner
#        # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock
#        # in hyperconverged settings where the volume is mounted on the same node as the osds.
#        csi.storage.k8s.io/fstype: ext4

cephFileSystems:
  - name: ceph-filesystem
    # see https://github.com/rook/rook/blob/master/Documentation/ceph-filesystem-crd.md#filesystem-settings for available configuration
    spec:
      metadataPool:
        replicated:
          size: 3
      dataPools:
        - failureDomain: host
          replicated:
            size: 3
      metadataServer:
        activeCount: 1
        activeStandby: true
    storageClass:
      enabled: true
      name: ceph-filesystem
      reclaimPolicy: Delete
      # see https://github.com/rook/rook/blob/master/Documentation/ceph-filesystem.md#provision-storage for available configuration
      parameters:
        # The secrets contain Ceph admin credentials.
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
        # Specify the filesystem type of the volume. If not specified, csi-provisioner
        # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock
        # in hyperconverged settings where the volume is mounted on the same node as the osds.
        csi.storage.k8s.io/fstype: ext4

cephObjectStores:
#  - name: object-store
#    # see https://github.com/rook/rook/blob/master/Documentation/ceph-object-store-crd.md#object-store-settings for available configuration
#    spec:
#      metadataPool:
#        failureDomain: host
#        replicated:
#          size: 3
#      dataPool:
#        failureDomain: host
#        erasureCoded:
#          dataChunks: 2
#          codingChunks: 1
#      preservePoolsOnDelete: false
#      gateway:
#        port: 80
#        # securePort: 443
#        # sslCertificateRef:
#        instances: 1
#      healthCheck:
#        bucket:
#          interval: 60s
#    storageClass:
#      enabled: true
#      name: ceph-object-storage
#      reclaimPolicy: Delete
#      # see https://github.com/rook/rook/blob/master/Documentation/ceph-object-bucket-claim.md#storageclass for available configuration
#      parameters:
#        # note: objectStoreNamespace and objectStoreName are configured by the chart
#        region: ap-northeast-2
